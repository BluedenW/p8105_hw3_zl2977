---
title: "Homework 3 solutions"
author: "Zhourong Li zl2977"
date: '`r format(Sys.time(), "%Y-%m-%d")`'
output: github_document
---


```{r setup, include=FALSE}
library(tidyverse)
library(p8105.datasets)

# knitr::opts_chunk$set(
# 	fig.width = 6, 
#   fig.asp = .6,
#   out.width = "90%"
# )

knitr::opts_chunk$set(echo = TRUE,
  warning = FALSE,
	fig.width = 10, 
  fig.height = 8,
  out.width = "90%")

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

### Due date

Due: October 10 at 10:00pm. 

### Points

| Problem         | Points    |
|:--------------- |:--------- |
| Problem 0       | 20        |
| Problem 1       | --        |
| Problem 2       | 40        |
| Problem 3       | 40        |
| Optional survey | No points |


### Problem 0

This "problem" focuses on structure of your submission, especially the use git and GitHub for reproducibility, R Projects to organize your work, R Markdown to write reproducible reports, relative paths to load data from local files, and reasonable naming structures for your files. 


### Problem 1

```{r}
data("instacart")
```

This dataset contains `r nrow(instacart)` rows and `r ncol(instacart)` columns. 

Observations are the level of items in orders by user. There are user / order variables -- user ID, order ID, order day, and order hour. There are also item variables -- name, aisle, department, and some numeric codes. 

How many aisles, and which are most items from?

```{r}
instacart %>% 
	count(aisle) %>% 
	arrange(desc(n))
```


Let's make a plot

```{r}
instacart %>% 
	count(aisle) %>% 
	filter(n > 10000) %>% 
	mutate(
		aisle = factor(aisle),
		aisle = fct_reorder(aisle, n)
	) %>% 
	ggplot(aes(x = aisle, y = n)) + 
	geom_point() + 
	theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```


Let's make a table!!

```{r}
instacart %>% 
	filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
	group_by(aisle) %>% 
	count(product_name) %>% 
	mutate(rank = min_rank(desc(n))) %>% 
	filter(rank < 4) %>% 
	arrange(aisle, rank) %>% 
	knitr::kable()
```


Apples vs ice cream..

```{r}
instacart %>% 
	filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
	group_by(product_name, order_dow) %>% 
	summarize(mean_hour = mean(order_hour_of_day)) %>% 
	pivot_wider(
		names_from = order_dow,
		values_from = mean_hour
	)
```

### Problem2

```{r,warning=FALSE,message=FALSE}
accel_data=
 read_csv(
    "./data/accel_data.csv"
  )%>%
  janitor::clean_names()%>%
  pivot_longer(
    activity_1:activity_1440,
    names_to="minute",
    names_prefix="activity_",
    values_to="activity_counts"
  )%>%
  mutate(
    minute=as.numeric(minute),
    day=factor(day),
    day=fct_relevel(day,"Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"),
    weekday_or_weekend=ifelse(day %in% c("Saturday","Sunday"),"weekend","weekday")
  )%>%
  arrange(week,day)

```
After tidying the dataset, our original dataset has become a `r dim(accel_data)` dataset, which contains `r nrow(accel_data)` observations of `r ncol(accel_data)` variables, each row records the information of that single minute in the five weeks. The variables include _week_, _day_id_, _day_, _minute_, _activity_counts_ and _weekday_or_weekend_. The data has been arranged by day of the the week from Sunday to Saturday.


```{r}
accel_data%>%
  group_by(week,day) %>%
  summarize(
    total_activity=sum(activity_counts)
  ) %>%
  pivot_wider(
    names_from = day,
    values_from=total_activity
  )

```
```{r}
accel_data %>%
  mutate(hour=minute / 60)%>%
  ggplot(aes(x = hour, y = activity_counts, group = day_id, color = day)) + 
  scale_x_discrete(
    breaks = seq(60,1440,60))+
  geom_point(alpha = .3, size = 1.6) + 
  geom_line(alpha = .3, size = 1.4) +
  geom_smooth(aes(group = day), se = FALSE)+
  labs(title = "24-hour activity time courses for each day ",
       caption = "Data from accel_data",
       x = "hour",
       y = "activity counts")+  
  scale_x_continuous(breaks = seq(0,24,2), limits = c(0,24))+
  theme(plot.title = element_text(hjust = 0.5))

```
                    
                    
Based on this graph, we can see the individual has relatively small activity between midnight until 5am everyday, which probably due to sleep. One peak in activity counts is in the middle of the day on Sundays, this peak might due to go out and eat lunch at weekend. Other peaks in activity counts are in between 7pm to 10pm on Friday and Saturday, these might  due to the individual go out eat dinner or do some exercises. There exists a high activity count around 7pm to 8pm on Wednesday as well. It seems like the individual has more activities during weekend comparing with weekdays.

### Problem3

```{r}
library(p8105.datasets)
data("ny_noaa")

```
The size of the dataset is `r dim(ny_noaa)`, there are `r nrow(ny_noaa)` observations of `ncol(ny_noaa)` variables, the variables include _id_, _prcp_, _snow_, _snwd_, _tmax_, _tmin_ and _date_. The type of variables include date, integer and character. Each row of the dataset contains the information about the weather in each station everyday. There are `r sum(is.na(ny_noaa$prcp))` missing values for precipitation, `r sum(is.na(ny_noaa$snow))` missing values for snowfall, `r sum(is.na(ny_noaa$snwd))` missing values for snow depth, `r sum(is.na(ny_noaa$tmax))` missing values for tmax, `r sum(is.na(ny_noaa$tmin))` missing values for tmin.

```{r}
noaa_data=ny_noaa %>%
  janitor::clean_names()%>%
  separate(date, into=c("year", "month", "day"), sep = "-")%>%
  mutate(tmax=as.numeric(tmax),
         tmin=as.numeric(tmin),
         prcp=prcp/10
         ) %>%
  mutate(tmax=tmax/10,
         tmin=tmin/10)

```
Observations for temperature, precipitation, and snowfall are set to reasonable units.

```{r}
noaa_data
mode(noaa_data$snow)
```

Use the _mode_ function in r, we find `r mode(noaa_data$snow)` is the most commonly observed value. This is probably due to snow is not common in New York for most of time.


```{r}
noaa_data %>%
  filter((month =="01"| month == "07") & !is.na(tmax)) %>%
  group_by(month, year, id)%>%
  summarize(average_temp=mean(tmax))%>%
  ggplot(aes(x = year, y = average_temp))+
  geom_boxplot()+
  facet_grid(.~month)+
  theme(axis.text.x=element_text(angle = 90, hjust =1),plot.title = element_text(hjust = 0.5)) +
  labs(
    title = "Average max temperature in January and in July in each station across years",
    x="Year",
    y="Average temperature"
  )
```
We can see the average max temperature of July is higher comparing with January. There are some outliers in the plots, which indicate that there existed some abnormally high and low temperatures in January, and there also existed some abnormally low temperatures in July.


```{r}
first_plot=
  noaa_data %>%
  filter(!is.na(tmax) & !is.na(tmin)) %>%
  ggplot(aes(x=tmax, y=tmin))+
  geom_hex()+
  labs(
    title = "tmax vs tmin for the full dataset",
    x="tmax",
    y="tmin")+
  theme(plot.title = element_text(hjust = 0.5), legend.position = "bottom", legend.key.width = unit(1.2,"cm"))


second_plot=
  noaa_data %>%
  filter(snow>0 & snow<100)%>%
  ggplot(aes(x=snow, color=year)) +
  geom_density(alpha = .5, adjust =.6)+
  labs(
    title = "Distribution of snowfall values",
    x = "Snowfall values",
    y = "Density") +
    theme(plot.title = element_text(hjust = 0.5), legend.position = "bottom", legend.key.width = unit(0.2, "cm"))

  
first_plot+second_plot   


```

From the hex plot, we found the largest counts are in area covered by (-15,30) of tmin and (-10, 30) of tmax. In the distribution of snowfall values, we observed that the most snowfall appears is 25mm while the least is 100mm. Although there are fluctuations, the trend is mainly decreasing from 0mm to 100mm. In the 0-15 mm range, there are significant differences of snowfall values between years.






















