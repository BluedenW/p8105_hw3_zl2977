Homework 3 solutions
================
Zhourong Li zl2977
2020-10-11

### Due date

Due: October 10 at 10:00pm.

### Points

| Problem         | Points    |
| :-------------- | :-------- |
| Problem 0       | 20        |
| Problem 1       | –         |
| Problem 2       | 40        |
| Problem 3       | 40        |
| Optional survey | No points |

### Problem 0

This “problem” focuses on structure of your submission, especially the
use git and GitHub for reproducibility, R Projects to organize your
work, R Markdown to write reproducible reports, relative paths to load
data from local files, and reasonable naming structures for your files.

### Problem 1

``` r
data("instacart")
```

This dataset contains 1384617 rows and 15 columns.

Observations are the level of items in orders by user. There are user /
order variables – user ID, order ID, order day, and order hour. There
are also item variables – name, aisle, department, and some numeric
codes.

How many aisles, and which are most items from?

``` r
instacart %>% 
    count(aisle) %>% 
    arrange(desc(n))
```

    ## # A tibble: 134 x 2
    ##    aisle                              n
    ##    <chr>                          <int>
    ##  1 fresh vegetables              150609
    ##  2 fresh fruits                  150473
    ##  3 packaged vegetables fruits     78493
    ##  4 yogurt                         55240
    ##  5 packaged cheese                41699
    ##  6 water seltzer sparkling water  36617
    ##  7 milk                           32644
    ##  8 chips pretzels                 31269
    ##  9 soy lactosefree                26240
    ## 10 bread                          23635
    ## # ... with 124 more rows

Let’s make a plot

``` r
instacart %>% 
    count(aisle) %>% 
    filter(n > 10000) %>% 
    mutate(
        aisle = factor(aisle),
        aisle = fct_reorder(aisle, n)
    ) %>% 
    ggplot(aes(x = aisle, y = n)) + 
    geom_point() + 
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

<img src="p8105_hw3_zl2977_files/figure-gfm/unnamed-chunk-3-1.png" width="90%" />

Let’s make a table\!\!

``` r
instacart %>% 
    filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
    group_by(aisle) %>% 
    count(product_name) %>% 
    mutate(rank = min_rank(desc(n))) %>% 
    filter(rank < 4) %>% 
    arrange(aisle, rank) %>% 
    knitr::kable()
```

| aisle                      | product\_name                                 |    n | rank |
| :------------------------- | :-------------------------------------------- | ---: | ---: |
| baking ingredients         | Light Brown Sugar                             |  499 |    1 |
| baking ingredients         | Pure Baking Soda                              |  387 |    2 |
| baking ingredients         | Cane Sugar                                    |  336 |    3 |
| dog food care              | Snack Sticks Chicken & Rice Recipe Dog Treats |   30 |    1 |
| dog food care              | Organix Chicken & Brown Rice Recipe           |   28 |    2 |
| dog food care              | Small Dog Biscuits                            |   26 |    3 |
| packaged vegetables fruits | Organic Baby Spinach                          | 9784 |    1 |
| packaged vegetables fruits | Organic Raspberries                           | 5546 |    2 |
| packaged vegetables fruits | Organic Blueberries                           | 4966 |    3 |

Apples vs ice cream..

``` r
instacart %>% 
    filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
    group_by(product_name, order_dow) %>% 
    summarize(mean_hour = mean(order_hour_of_day)) %>% 
    pivot_wider(
        names_from = order_dow,
        values_from = mean_hour
    )
```

    ## `summarise()` regrouping output by 'product_name' (override with `.groups` argument)

    ## # A tibble: 2 x 8
    ## # Groups:   product_name [2]
    ##   product_name       `0`   `1`   `2`   `3`   `4`   `5`   `6`
    ##   <chr>            <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>
    ## 1 Coffee Ice Cream  13.8  14.3  15.4  15.3  15.2  12.3  13.8
    ## 2 Pink Lady Apples  13.4  11.4  11.7  14.2  11.6  12.8  11.9

### Problem2

``` r
accel_data=
 read_csv(
    "./data/accel_data.csv"
  )%>%
  janitor::clean_names()%>%
  pivot_longer(
    activity_1:activity_1440,
    names_to="minute",
    names_prefix="activity_",
    values_to="activity_counts"
  )%>%
  mutate(
    minute=as.numeric(minute),
    day=factor(day),
    day=fct_relevel(day,"Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"),
    weekday_or_weekend=ifelse(day %in% c("Saturday","Sunday"),"weekend","weekday")
  )%>%
  arrange(week,day)
```

After tidying the dataset, our original dataset has become a 50400, 6
dataset, which contains 50400 observations of 6 variables, each row
records the information of that single minute in the five weeks. The
variables include *week*, *day\_id*, *day*, *minute*, *activity\_counts*
and *weekday\_or\_weekend*. The data has been arranged by day of the the
week from Sunday to Saturday.

``` r
accel_data%>%
  group_by(week,day) %>%
  summarize(
    total_activity=sum(activity_counts)
  ) %>%
  pivot_wider(
    names_from = day,
    values_from=total_activity
  )
```

    ## `summarise()` regrouping output by 'week' (override with `.groups` argument)

    ## # A tibble: 5 x 8
    ## # Groups:   week [5]
    ##    week Sunday  Monday Tuesday Wednesday Thursday  Friday Saturday
    ##   <dbl>  <dbl>   <dbl>   <dbl>     <dbl>    <dbl>   <dbl>    <dbl>
    ## 1     1 631105  78828. 307094.   340115.  355924. 480543.   376254
    ## 2     2 422018 295431  423245    440962   474048  568839    607175
    ## 3     3 467052 685910  381507    468869   371230  467420    382928
    ## 4     4 260617 409450  319568    434460   340291  154049      1440
    ## 5     5 138421 389080  367824    445366   549658  620860      1440

``` r
accel_data %>%
  mutate(hour=minute / 60)%>%
  ggplot(aes(x = hour, y = activity_counts, group = day_id, color = day)) + 
  scale_x_discrete(
    breaks = seq(60,1440,60))+
  geom_point(alpha = .3, size = 1.6) + 
  geom_line(alpha = .3, size = 1.4) +
  geom_smooth(aes(group = day), se = FALSE)+
  labs(title = "24-hour activity time courses for each day ",
       caption = "Data from accel_data",
       x = "hour",
       y = "activity counts")+  
  scale_x_continuous(breaks = seq(0,24,2), limits = c(0,24))+
  theme(plot.title = element_text(hjust = 0.5))
```

    ## Scale for 'x' is already present. Adding another scale for 'x', which will
    ## replace the existing scale.

    ## `geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = "cs")'

<img src="p8105_hw3_zl2977_files/figure-gfm/unnamed-chunk-8-1.png" width="90%" />

Based on this graph, we can see the individual has relatively small
activity between midnight until 5am everyday, which probably due to
sleep. One peak in activity counts is in the middle of the day on
Sundays, this peak might due to go out and eat lunch at weekend. Other
peaks in activity counts are in between 7pm to 10pm on Friday and
Saturday, these might due to the individual go out eat dinner or do some
exercises. There exists a high activity count around 7pm to 8pm on
Wednesday as well. It seems like the individual has more activities
during weekend comparing with weekdays.

### Problem3

``` r
library(p8105.datasets)
data("ny_noaa")
```

The size of the dataset is 2595176, 7, there are 2595176 observations of
`ncol(ny_noaa)` variables, the variables include *id*, *prcp*, *snow*,
*snwd*, *tmax*, *tmin* and *date*. The type of variables include date,
integer and character. Each row of the dataset contains the information
about the weather in each station everyday. There are 145838 missing
values for precipitation, 381221 missing values for snowfall, 591786
missing values for snow depth, 1134358 missing values for tmax, 1134420
missing values for tmin.

``` r
noaa_data=ny_noaa %>%
  janitor::clean_names()%>%
  separate(date, into=c("year", "month", "day"), sep = "-")%>%
  mutate(tmax=as.numeric(tmax),
         tmin=as.numeric(tmin),
         prcp=prcp/10
         ) %>%
  mutate(tmax=tmax/10,
         tmin=tmin/10)
```

Observations for temperature, precipitation, and snowfall are set to
reasonable units.

``` r
noaa_data
```

    ## # A tibble: 2,595,176 x 9
    ##    id          year  month day    prcp  snow  snwd  tmax  tmin
    ##    <chr>       <chr> <chr> <chr> <dbl> <int> <int> <dbl> <dbl>
    ##  1 US1NYAB0001 2007  11    01       NA    NA    NA    NA    NA
    ##  2 US1NYAB0001 2007  11    02       NA    NA    NA    NA    NA
    ##  3 US1NYAB0001 2007  11    03       NA    NA    NA    NA    NA
    ##  4 US1NYAB0001 2007  11    04       NA    NA    NA    NA    NA
    ##  5 US1NYAB0001 2007  11    05       NA    NA    NA    NA    NA
    ##  6 US1NYAB0001 2007  11    06       NA    NA    NA    NA    NA
    ##  7 US1NYAB0001 2007  11    07       NA    NA    NA    NA    NA
    ##  8 US1NYAB0001 2007  11    08       NA    NA    NA    NA    NA
    ##  9 US1NYAB0001 2007  11    09       NA    NA    NA    NA    NA
    ## 10 US1NYAB0001 2007  11    10       NA    NA    NA    NA    NA
    ## # ... with 2,595,166 more rows

``` r
mode(noaa_data$snow)
```

    ## [1] "numeric"

Use the *mode* function in r, we find numeric is the most commonly
observed value. This is probably due to snow is not common in New York
for most of time.

``` r
noaa_data %>%
  filter((month =="01"| month == "07") & !is.na(tmax)) %>%
  group_by(month, year, id)%>%
  summarize(average_temp=mean(tmax))%>%
  ggplot(aes(x = year, y = average_temp))+
  geom_boxplot()+
  facet_grid(.~month)+
  theme(axis.text.x=element_text(angle = 90, hjust =1),plot.title = element_text(hjust = 0.5)) +
  labs(
    title = "Average max temperature in January and in July in each station across years",
    x="Year",
    y="Average temperature"
  )
```

    ## `summarise()` regrouping output by 'month', 'year' (override with `.groups` argument)

<img src="p8105_hw3_zl2977_files/figure-gfm/unnamed-chunk-12-1.png" width="90%" />
We can see the average max temperature of July is higher comparing with
January. There are some outliers in the plots, which indicate that there
existed some abnormally high and low temperatures in January, and there
also existed some abnormally low temperatures in July.

``` r
first_plot=
  noaa_data %>%
  filter(!is.na(tmax) & !is.na(tmin)) %>%
  ggplot(aes(x=tmax, y=tmin))+
  geom_hex()+
  labs(
    title = "tmax vs tmin for the full dataset",
    x="tmax",
    y="tmin")+
  theme(plot.title = element_text(hjust = 0.5),legend.position = "bottom", legend.key.width = unit(1.2,"cm"))


second_plot=
  noaa_data %>%
  filter(snow>0 & snow<100)%>%
  ggplot(aes(x=snow, color=year)) +
  geom_density(alpha = .5, adjust =.6)+
  labs(
    title = "Distribution of snowfall values",
    x = "Snowfall values",
    y = "Density") +
    theme(plot.title = element_text(hjust = 0.5), legend.position = "bottom", legend.key.width = unit(0.2, "cm"))

  
first_plot+second_plot   
```

<img src="p8105_hw3_zl2977_files/figure-gfm/unnamed-chunk-13-1.png" width="90%" />

From the hex plot, we found the largest counts are in area covered by
(-15,30) of tmin and (-10, 30) of tmax. In the distribution of snowfall
values, we observed that the most snowfall appears is 25mm while the
least is 100mm. Although there are fluctuations, the trend is mainly
decreasing from 0mm to 100mm. In the 0-15 mm range, there are
significant differences of snowfall values between years.
